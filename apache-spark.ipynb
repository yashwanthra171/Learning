{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## APACHE PYSPARK","metadata":{}},{"cell_type":"markdown","source":"## WHY SPARK \n**Apache spark is written in SCALA**\n- faster\n- scalable\n- supoorts many different languages\n- has its own ecosystem\n\nWhile working on Big data we wanted code to run fast, which is not satisfied by many other languages, **spark runs on JVM and connects with other Java based BIG DATA systems such as cassandra, HBase and HDFS (these are more like DB's)**\n\n- Spark runs locally or in a cluster(that can be on-premise or a cloud)\n- Runs in top of Hadoop, Yarn , Apache mesos and a cloud\n- It is designed to run on large distributed clusters , but can also run on single machine\n- **Spark core API** is the foundation for the spark ecosystem\n- Spark ecosystem has 1)spark SQL, 2)streaming (for real time data) 3)MLlib (for ML) 4)GraphX (for graph structed data)\n\n**What Spark core API does ?**\n- task scheduling\n- memory management\n- fault recovery\n- interacting with storage systems\n- NOTE: Scalar is the default language\n\n**Spark SQL and data frames**\n- Spark allows us to do EDA using SQL ( via. DF programming abstraction )\n- Allows complex analytics with SQL\n\n**Spark Streaming**\n- process & analyze real time data or historic data\n- code used for batch data can be used in real time data\n\n**MLlib**\n- gives scalable ML algo's \n- 100 times faster than map reduce\n\n**GraphX**\n- a graph computation engine that allows to work with graph structured data\n- a new graph abstraction :: a directed multi graph, with properties attached with all vertices edges\n\n**What is pySpark**\n- A python wrapper around the spark\n- but we have pandas, hadoop and Dask that work similarly :: Then why spark\n    - Pandas :: for tabular data, mature and feature rich , but limited to single machine ( won't be able to store data across differnt machines )\n    - hadoop :: a distributed system, where compute system was there in Map reduce and storage system in HDFS (closely intergrated ie. can't you a cloud storage to do computations here )\n        - the chances of failure is more & at that time Hadoop read and writes it to another cluster, whereas spark works on memory \n        - Hadoop requires more memory on disk, spark requires more RAM (setting up spark is expensive)\n    - Dask :: a lib for parallel computing in python\n        - Built on python and supports only python\n        - no SQL support, DASK only supports python MLlibs, don't have graphX computation support\n\n**How spark emerged ?**\n- Hadoop map reduce was inefficient for iterative and interactive computing tasks\n- Amp lab created spark in 2013 and gave it to Apache \n- Then the creaters of spark , brought Databricks to the table\n\n**What's special with Databricks version of spark**\n- It provides interactive notebooks\n- Gives optimized version of Apache spark\n- Gives enterpise security for organizations\n- Other business intelligence tools can be conncted using databricks\n\n**Spark Components : Driver, worker nodes, cluster manager**\n- DRIVER\n    - Contains info about program \n    - controls or distributes the work to worker nodes that has executors\n    - the executors in turn will return the result back to driver\n- Cluster manager : allocates the resources to the driver\n- Cluster manager types: 1)standalone 2)Apache mesos 3)hadoop yarn and 4) kubernetes\n\n**Working with spark**\n- need to create a spark session first\n- spark session is a single unified entry point to manipulate spark\n\n**When a spark session is started in python :: in backgroud the py4j (python interpreter access the JVM & java objects)**\n   \n**Partitions :** As spark is a disributed system, so to work in parallel spark need to break the data into **Chunks or Partitions** - This is a set of rows distributed among clusters\nNO PARALLELIZATION :: when there is only one partion but has many worker nodes OR when there is several partition but only one worker\n\n**Transformation :** core data structure in spark that are immutable (can't be changed ), instructes used to modify the data frame are called transformations. When transformation are performed **nothing seems to happen** to actually do them we need something called ACTION operations\n\n**Lazy Evaluation :** When transformations are done , spark won't do them right away, instead there is a plan of transformations to be performed on the data, this waiting until last moment to do the work is called lazy evaluation . This is helpful as it creates a streamline plan and useful in many big data jobs\n\n**Actions :** Three type of actions 1) view data in the console eg: .show() 2) collect data eg: .collect() - collects the data to the driver 3)write data to output data sources eg: .write.format()","metadata":{}},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:40:34.599341Z","iopub.execute_input":"2022-04-12T16:40:34.600567Z","iopub.status.idle":"2022-04-12T16:40:35.391667Z","shell.execute_reply.started":"2022-04-12T16:40:34.60042Z","shell.execute_reply":"2022-04-12T16:40:35.390454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get update\n!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n!tar xf spark-2.3.1-bin-hadoop2.7.tgz\n!pip install -q findspark\n!pip install py4j","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:40:35.773206Z","iopub.execute_input":"2022-04-12T16:40:35.773624Z","iopub.status.idle":"2022-04-12T16:41:40.625389Z","shell.execute_reply.started":"2022-04-12T16:40:35.773583Z","shell.execute_reply":"2022-04-12T16:41:40.624047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:41:40.628493Z","iopub.execute_input":"2022-04-12T16:41:40.62889Z","iopub.status.idle":"2022-04-12T16:41:41.381072Z","shell.execute_reply.started":"2022-04-12T16:41:40.628839Z","shell.execute_reply":"2022-04-12T16:41:41.379991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:41:41.382672Z","iopub.execute_input":"2022-04-12T16:41:41.382951Z","iopub.status.idle":"2022-04-12T16:41:42.133846Z","shell.execute_reply.started":"2022-04-12T16:41:41.382917Z","shell.execute_reply":"2022-04-12T16:41:42.13297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Setting up environment","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \"/kaggle/working/spark-2.3.1-bin-hadoop2.7\"","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:41:42.136149Z","iopub.execute_input":"2022-04-12T16:41:42.136969Z","iopub.status.idle":"2022-04-12T16:41:42.143062Z","shell.execute_reply.started":"2022-04-12T16:41:42.136921Z","shell.execute_reply":"2022-04-12T16:41:42.141737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import findspark\nfindspark.init()\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom pyspark import SparkContext\n\nsc = SparkContext.getOrCreate()\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate() \nspark","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:41:42.144992Z","iopub.execute_input":"2022-04-12T16:41:42.145536Z","iopub.status.idle":"2022-04-12T16:41:46.427214Z","shell.execute_reply.started":"2022-04-12T16:41:42.145489Z","shell.execute_reply":"2022-04-12T16:41:46.426189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Downloading the data","metadata":{}},{"cell_type":"code","source":"# !wget https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\n# renaming the downloaded file\n# !mv rows.csv\\?accessType\\=DOWNLOAD reported_crimes.csv","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:02.156635Z","iopub.execute_input":"2022-04-06T09:56:02.157369Z","iopub.status.idle":"2022-04-06T09:56:02.161469Z","shell.execute_reply.started":"2022-04-06T09:56:02.157325Z","shell.execute_reply":"2022-04-06T09:56:02.160653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:02.162938Z","iopub.execute_input":"2022-04-06T09:56:02.163471Z","iopub.status.idle":"2022-04-06T09:56:02.941584Z","shell.execute_reply.started":"2022-04-06T09:56:02.163436Z","shell.execute_reply":"2022-04-06T09:56:02.940561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import to_timestamp, col, lit\nrc = spark.read.csv('../input/reported-crime/Crimes_-_2001_to_Present.csv', header = True).withColumn('Date',to_timestamp(col('Date'), 'mm/dd/yyyy hh:mm:ss a')).filter(col('Date') <= '2018-11-11')\nrc.show(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:02.945767Z","iopub.execute_input":"2022-04-06T09:56:02.946139Z","iopub.status.idle":"2022-04-06T09:56:20.256979Z","shell.execute_reply.started":"2022-04-06T09:56:02.9461Z","shell.execute_reply":"2022-04-06T09:56:20.256256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### DataFrame API\n- Two main API's are learned here\n- Dataframe API [High level API's]\n- Resilient Distributed DataFrames (RDD) [Low level API's]\n\nIn intial times spark made distributed data processing using RDD's [ a simple API for distributed data processing ]\nIn Spark :: A dataframe is a distributed collection of object of type ROW, A dataframe can be created using structed data/HIVE/external RDD's\n\n- Dataset API\n    - This is only supported by statically typed language\n    - Python - a dynamically typed language , this is not supported by it \n    ","metadata":{}},{"cell_type":"markdown","source":"#### Working with DataFrame","metadata":{}},{"cell_type":"code","source":"# temp\n# Using pandas\n# import pandas as pd\n# df = pd.read_Csv(fileName) # to load a file\n# df.head(3) # to view the df\n\n\nfrom pyspark.sql import SparkSession\nspark_session = SparkSession.builder.getOrCreate() # creating a spark session\ndf= spark_session.read.csv('../input/reported-crime/Crimes_-_2001_to_Present.csv', header = True) # to load a csv file\n# df.take(3) # this will return a list of row object, this internally calls collect() with limit()\n# df.collect() # will return all the rows of the df, may cause the driver node to crash for large datasets\ndf.show(2) # this will return rows with a good display or nice format\n# df.limit(3) # will return a new dataframe with the specified number of rows\n# df.head(3) # this will return an array of len 3, head() in turn calls the take()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:20.258214Z","iopub.execute_input":"2022-04-06T09:56:20.259273Z","iopub.status.idle":"2022-04-06T09:56:25.884982Z","shell.execute_reply.started":"2022-04-06T09:56:20.25922Z","shell.execute_reply":"2022-04-06T09:56:25.884177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Schemas\n\n- spark can infer the schema by default [by looking into the couple of rows and determining the data type]\n- But in a production environment, we need to explicitly define the schemas\n- Defining types in pyspark is easy \n- import pyspark.sql.types\n- schema is a struct type, which in turn contain three elements (name of the column, type of the column, whether the column can have null values or not (true/false), metadata of the column (its optional))\n","metadata":{}},{"cell_type":"code","source":"# In pandas :: df.dtypes\ndf.printSchema(), df.dtypes # both are same except only the way/style it is getting displayed differs","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:25.888014Z","iopub.execute_input":"2022-04-06T09:56:25.888494Z","iopub.status.idle":"2022-04-06T09:56:25.943382Z","shell.execute_reply.started":"2022-04-06T09:56:25.888462Z","shell.execute_reply":"2022-04-06T09:56:25.942656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing \ndf = df.withColumn('Date', to_timestamp(col('Date'), 'mm/dd/yyyy hh:mm:ss a')).filter(col('Date') <= '2018-11-11')\ndf.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:25.944533Z","iopub.execute_input":"2022-04-06T09:56:25.944983Z","iopub.status.idle":"2022-04-06T09:56:26.036697Z","shell.execute_reply.started":"2022-04-06T09:56:25.944944Z","shell.execute_reply":"2022-04-06T09:56:26.035792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:26.03797Z","iopub.execute_input":"2022-04-06T09:56:26.038415Z","iopub.status.idle":"2022-04-06T09:56:26.052147Z","shell.execute_reply.started":"2022-04-06T09:56:26.038376Z","shell.execute_reply":"2022-04-06T09:56:26.051014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# explictly defining the data types in spark\nfrom pyspark.sql.types import StructType, StructField, StringType, TimestampType, BooleanType, DoubleType, IntegerType\nlabels = [\n    ('ID', StringType()),\n ('Case Number',StringType()),\n ('Date',TimestampType()),\n ('Block',StringType()),\n ('IUCR',StringType()),\n ('Primary Type',StringType()),\n ('Description',StringType()),\n ('Location Description',StringType()),\n ('Arrest',StringType()),\n ('Domestic',BooleanType()),\n ('Beat',StringType()),\n ('District',StringType()),\n ('Ward',StringType()),\n ('Community Area',StringType()),\n ('FBI Code',StringType()),\n ('X Coordinate',StringType()),\n ('Y Coordinate',StringType()),\n ('Year',IntegerType()),\n ('Updated On',TimestampType()),\n ('Latitude',DoubleType()),\n ('Longitude',DoubleType()),\n ('Location', StringType())\n]\n\n\n# iterating over the labels list \nschema = StructType([StructField (x[0],x[1],True) for x in labels ])\nschema","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:26.054618Z","iopub.execute_input":"2022-04-06T09:56:26.055457Z","iopub.status.idle":"2022-04-06T09:56:26.077583Z","shell.execute_reply.started":"2022-04-06T09:56:26.055403Z","shell.execute_reply":"2022-04-06T09:56:26.076464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rc = spark_session.read.csv('../input/reported-crime/Crimes_-_2001_to_Present.csv', schema= schema, header = True)\nrc.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:26.07942Z","iopub.execute_input":"2022-04-06T09:56:26.080146Z","iopub.status.idle":"2022-04-06T09:56:26.128976Z","shell.execute_reply.started":"2022-04-06T09:56:26.080073Z","shell.execute_reply":"2022-04-06T09:56:26.128202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rc.show(3) # these null values mean that there are some values which can't be converted into desired data type as specified in the schema","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:26.132205Z","iopub.execute_input":"2022-04-06T09:56:26.132554Z","iopub.status.idle":"2022-04-06T09:56:34.725868Z","shell.execute_reply.started":"2022-04-06T09:56:26.132518Z","shell.execute_reply":"2022-04-06T09:56:34.724936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Working with Columns","metadata":{}},{"cell_type":"code","source":"# ACCESSING A COLUMN\n# In both python and pyspark :: df.columns will give list of column names\n# In pandas :: df.column_name, df['column_name'] returns the entire column\n# In pyspark :: df.select(col('column_name')) will return the column\n\n# FOR MULTIPLE COLUMN SELECTION\n# In pandas :: df[['column_name1', 'column_name2']]\n# In pyspark :: df.select(column_name1, column_name2).show(3)\n\n# ADDING A NEW COLUMN\n# IN pandas :: df['new_column'] = df['old_column'] * 2\n# In pyspark :: df.withColumn('new_column', 2*df['old_column'])\n\n# RENAMING THE COLUMN \n# In pandas :: df.rename(columns = {'oldName':'newName'}, inplace = True)\n# In pyspark :: df.withColumnRenamed('oldName', 'newName') -- this will return a new dataframe\n\n# DROPPING A COLUMN IN BOTH PYSPARK AND PANDAS ARE SAME\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:34.727019Z","iopub.execute_input":"2022-04-06T09:56:34.727304Z","iopub.status.idle":"2022-04-06T09:56:34.732498Z","shell.execute_reply.started":"2022-04-06T09:56:34.727271Z","shell.execute_reply":"2022-04-06T09:56:34.731557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select('Date').show(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:34.734115Z","iopub.execute_input":"2022-04-06T09:56:34.734797Z","iopub.status.idle":"2022-04-06T09:56:40.384715Z","shell.execute_reply.started":"2022-04-06T09:56:34.734718Z","shell.execute_reply":"2022-04-06T09:56:40.383889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select('Case Number', 'Date', 'Arrest').show(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:40.385833Z","iopub.execute_input":"2022-04-06T09:56:40.386081Z","iopub.status.idle":"2022-04-06T09:56:46.026745Z","shell.execute_reply.started":"2022-04-06T09:56:40.38605Z","shell.execute_reply":"2022-04-06T09:56:46.025744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select(df.IUCR).show(3), df.select(col('Updated On')).show(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:46.027841Z","iopub.execute_input":"2022-04-06T09:56:46.028191Z","iopub.status.idle":"2022-04-06T09:56:57.167593Z","shell.execute_reply.started":"2022-04-06T09:56:46.028146Z","shell.execute_reply":"2022-04-06T09:56:57.166985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding a column with constant value in all the rows\nfrom pyspark.sql.functions import lit\ndf.withColumn('ONEs', lit(1)).show(3) # not an inplace function","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:56:57.168467Z","iopub.execute_input":"2022-04-06T09:56:57.169251Z","iopub.status.idle":"2022-04-06T09:57:04.573172Z","shell.execute_reply.started":"2022-04-06T09:56:57.169217Z","shell.execute_reply":"2022-04-06T09:57:04.572483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop('IUCR').show(3) # not an inplace function ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:57:04.574006Z","iopub.execute_input":"2022-04-06T09:57:04.574253Z","iopub.status.idle":"2022-04-06T09:57:11.890521Z","shell.execute_reply.started":"2022-04-06T09:57:04.574218Z","shell.execute_reply":"2022-04-06T09:57:11.889444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Working with rows","metadata":{}},{"cell_type":"code","source":"# FILTERING ROWS\n# In pandas ::  df[df.column_name > condition ]\n# In pyspark :: df.filter(col('columns_name') >= condition)\n\n# UNIQUE ROWS OF a DF\n# In pandas :: df.column_name.unique()\n# In pyspark :: df.select('column_name').distinct().show()\n\n# SORTING ROWS\n# In Pandas :: df.column_name.sort_values()\n# In pyspark :: df.orderby('column_name')\n\n# Appending two DF\n# In pandas :: df.concat([df1, df2])\n# In pyspark :: df.union(df1) - make sure that both df & df1 has same schema and same number of cols\n","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:57:11.892724Z","iopub.execute_input":"2022-04-06T09:57:11.893405Z","iopub.status.idle":"2022-04-06T09:57:11.897948Z","shell.execute_reply.started":"2022-04-06T09:57:11.893367Z","shell.execute_reply":"2022-04-06T09:57:11.896889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Add the reported crimes for an additional day, 12-Nov-2018, to our dataset.**","metadata":{}},{"cell_type":"code","source":"# can't use df here , since df itself is a filtered DF\none_day = spark_session.read.csv('../input/reported-crime/Crimes_-_2001_to_Present.csv', header = True).withColumn('Date', to_timestamp(col('Date'), 'mm/dd/yyyy hh:mm:ss a'))#.filter(col('Date') == lit('2018-11-15'))\none_day.count()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:57:11.900008Z","iopub.execute_input":"2022-04-06T09:57:11.900451Z","iopub.status.idle":"2022-04-06T09:57:30.760879Z","shell.execute_reply.started":"2022-04-06T09:57:11.900402Z","shell.execute_reply":"2022-04-06T09:57:30.759962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"one_day.filter(col('Date') > lit('2018-11-11')).select('Date').distinct().show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:57:30.762684Z","iopub.execute_input":"2022-04-06T09:57:30.76301Z","iopub.status.idle":"2022-04-06T09:58:10.935877Z","shell.execute_reply.started":"2022-04-06T09:57:30.762968Z","shell.execute_reply":"2022-04-06T09:58:10.93426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interval = one_day.filter((col('Date') > lit('2018-11-11')) & (col('Date')<= lit('2019-01-30')))\ninterval.count()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:58:10.94223Z","iopub.execute_input":"2022-04-06T09:58:10.948254Z","iopub.status.idle":"2022-04-06T09:58:49.539Z","shell.execute_reply.started":"2022-04-06T09:58:10.948174Z","shell.execute_reply":"2022-04-06T09:58:49.537961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.count()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:58:49.540545Z","iopub.execute_input":"2022-04-06T09:58:49.541925Z","iopub.status.idle":"2022-04-06T09:59:24.910469Z","shell.execute_reply.started":"2022-04-06T09:58:49.54186Z","shell.execute_reply":"2022-04-06T09:59:24.909711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined = df.union(interval)\nassert combined.count() == (df.count() + interval.count())","metadata":{"execution":{"iopub.status.busy":"2022-04-06T09:59:24.9115Z","iopub.execute_input":"2022-04-06T09:59:24.911703Z","iopub.status.idle":"2022-04-06T10:01:46.905371Z","shell.execute_reply.started":"2022-04-06T09:59:24.911677Z","shell.execute_reply":"2022-04-06T10:01:46.90427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined = combined.orderBy('Date')\ncombined.select('Date').show(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T10:01:46.910346Z","iopub.execute_input":"2022-04-06T10:01:46.911305Z","iopub.status.idle":"2022-04-06T10:03:07.452781Z","shell.execute_reply.started":"2022-04-06T10:01:46.91125Z","shell.execute_reply":"2022-04-06T10:03:07.451227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Top 10 reported crimes in the order of primary type in desc order**","metadata":{}},{"cell_type":"code","source":"combined.sort(col('Primary Type').desc()).select('Primary Type').show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T10:03:07.454062Z","iopub.execute_input":"2022-04-06T10:03:07.455158Z","iopub.status.idle":"2022-04-06T10:05:59.461851Z","shell.execute_reply.started":"2022-04-06T10:03:07.454922Z","shell.execute_reply":"2022-04-06T10:05:59.46074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What are the top 10 number of reported crimes by Primary type, in descending order of occurence?**","metadata":{}},{"cell_type":"code","source":"combined.orderBy(col('Primary Type').desc()).select('Primary Type').distinct().show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T10:05:59.463436Z","iopub.execute_input":"2022-04-06T10:05:59.463775Z","iopub.status.idle":"2022-04-06T10:08:58.017276Z","shell.execute_reply.started":"2022-04-06T10:05:59.463733Z","shell.execute_reply":"2022-04-06T10:08:58.016145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the above cmd will give only the \"Primary Type\" in desc order (alphabetically -\n# not based on thier number of occurance)\ncombined.groupBy('Primary Type').count().orderBy('count', ascending = False).show(10) # after a groupby its essential to have a aggregate function","metadata":{"execution":{"iopub.status.busy":"2022-04-06T10:08:58.019467Z","iopub.execute_input":"2022-04-06T10:08:58.019827Z","iopub.status.idle":"2022-04-06T10:11:50.314438Z","shell.execute_reply.started":"2022-04-06T10:08:58.019782Z","shell.execute_reply":"2022-04-06T10:11:50.313273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**1) What percent of reported crime resulted in arrest**\n**2) What are the top 3 locations of the reported crimes**","metadata":{}},{"cell_type":"code","source":"#1\ncombined.fillna('unknown', subset = ['Arrest'])\ncombined.select('Arrest').distinct().show()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T10:11:50.316144Z","iopub.execute_input":"2022-04-06T10:11:50.31653Z","iopub.status.idle":"2022-04-06T10:14:41.821687Z","shell.execute_reply.started":"2022-04-06T10:11:50.316484Z","shell.execute_reply":"2022-04-06T10:14:41.820667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(combined.filter(col('Arrest') == 'true').count() / combined.count())*100","metadata":{"execution":{"iopub.status.busy":"2022-04-06T10:14:41.823064Z","iopub.execute_input":"2022-04-06T10:14:41.823963Z","iopub.status.idle":"2022-04-06T10:19:55.725552Z","shell.execute_reply.started":"2022-04-06T10:14:41.823907Z","shell.execute_reply":"2022-04-06T10:19:55.724618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2\ncombined.groupBy('Location Description').count().orderBy('count', ascending = False).show(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T10:25:57.894441Z","iopub.execute_input":"2022-04-06T10:25:57.895182Z","iopub.status.idle":"2022-04-06T10:28:48.684372Z","shell.execute_reply.started":"2022-04-06T10:25:57.89514Z","shell.execute_reply":"2022-04-06T10:28:48.683496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import to_timestamp, col, lit\ncombined = spark.read.csv('../input/reported-crime/Crimes_-_2001_to_Present.csv', header = True).withColumn('Date',to_timestamp(col('Date'), 'mm/dd/yyyy hh:mm:ss a'))#.filter(col('Date') <= '2018-11-11')\ncombined.show(2)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T13:54:23.363273Z","iopub.execute_input":"2022-04-07T13:54:23.364634Z","iopub.status.idle":"2022-04-07T13:54:35.114951Z","shell.execute_reply.started":"2022-04-07T13:54:23.364556Z","shell.execute_reply":"2022-04-07T13:54:35.114013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- groupBy in multiple columns and aggregation in multiple columns","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import countDistinct\ncombined.groupBy('Location Description').agg(countDistinct(col('Primary Type')).alias('numberOf_PrimaryType'), pyspark.sql.functions.count(col('Location Description')).alias('LocationCount')).orderBy(col('LocationCount'), ascending = False).show(3) #.count().orderBy('count', ascending = False).show(3)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T13:54:35.11678Z","iopub.execute_input":"2022-04-07T13:54:35.117044Z","iopub.status.idle":"2022-04-07T13:54:58.093938Z","shell.execute_reply.started":"2022-04-07T13:54:35.117012Z","shell.execute_reply":"2022-04-07T13:54:58.092978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Built-in functions with pyspark","metadata":{}},{"cell_type":"markdown","source":"- Built in functions are available in pyspark.sql.functions\n- dir(functions) - will give you the list of available functions ","metadata":{}},{"cell_type":"code","source":"import pyspark.sql.functions as func\nprint(dir(func))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:05:55.005785Z","iopub.execute_input":"2022-04-06T15:05:55.006326Z","iopub.status.idle":"2022-04-06T15:05:55.012293Z","shell.execute_reply.started":"2022-04-06T15:05:55.006285Z","shell.execute_reply":"2022-04-06T15:05:55.011366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using string built-in functions :: Print lower and upper case of \"Primary Type\" column and first 4 characters of that column**","metadata":{}},{"cell_type":"code","source":"help(func.lower)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:05:58.044008Z","iopub.execute_input":"2022-04-06T15:05:58.044851Z","iopub.status.idle":"2022-04-06T15:05:58.050666Z","shell.execute_reply.started":"2022-04-06T15:05:58.044808Z","shell.execute_reply":"2022-04-06T15:05:58.049768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(func.substring) # note it starts with 1 as the first index not zero ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:06:06.78451Z","iopub.execute_input":"2022-04-06T15:06:06.785465Z","iopub.status.idle":"2022-04-06T15:06:06.791102Z","shell.execute_reply.started":"2022-04-06T15:06:06.785402Z","shell.execute_reply":"2022-04-06T15:06:06.790467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined.select(func.lower(col('Primary Type')).alias('lower_Primary_type'), func.upper(col('Primary Type')).alias('upper_Primary_type'), func.substring(col('Primary Type'), 1, 4).alias('substring_Primary_type')).show(4)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:18:26.007216Z","iopub.execute_input":"2022-04-06T15:18:26.007676Z","iopub.status.idle":"2022-04-06T15:18:28.565574Z","shell.execute_reply.started":"2022-04-06T15:18:26.007629Z","shell.execute_reply":"2022-04-06T15:18:28.564742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For numeric built-in functions :: oldest data and most recent date in Date column**","metadata":{}},{"cell_type":"code","source":"combined.select(func.min(col('Date')).alias('oldest_date'), func.max(col('Date')).alias('recent_date')).show(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:19:03.286151Z","iopub.execute_input":"2022-04-06T15:19:03.286551Z","iopub.status.idle":"2022-04-06T15:19:51.167608Z","shell.execute_reply.started":"2022-04-06T15:19:03.286505Z","shell.execute_reply":"2022-04-06T15:19:51.166639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What is 3 days earlier oldest date and 3 days later from recent date**","metadata":{}},{"cell_type":"code","source":"help(func.date_add)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T15:19:51.169507Z","iopub.execute_input":"2022-04-06T15:19:51.170525Z","iopub.status.idle":"2022-04-06T15:19:51.177741Z","shell.execute_reply.started":"2022-04-06T15:19:51.170473Z","shell.execute_reply":"2022-04-06T15:19:51.176801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined.select(func.date_add(func.min(col('Date')), 3).alias('3DaysEarlier'),func.date_sub(func.max(col('Date')), 3).alias('3DaysLater')).show(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T16:16:43.855412Z","iopub.execute_input":"2022-04-06T16:16:43.856591Z","iopub.status.idle":"2022-04-06T16:17:12.294839Z","shell.execute_reply.started":"2022-04-06T16:16:43.856524Z","shell.execute_reply":"2022-04-06T16:17:12.293831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Working with dates\n- There exist several formats to specify the date identifying some ways of using it in pyspark\n- the to_date() and to_timestamp() -- are more like changing whatever given in the string to be converted into standard format [yyyy-MM-dd HH:mm:ss] ","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import to_date, to_timestamp, lit\ndf = spark.createDataFrame([('2020-12-11 13:30:00',)], ['Bday'])\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T14:05:43.310382Z","iopub.execute_input":"2022-04-07T14:05:43.310691Z","iopub.status.idle":"2022-04-07T14:05:44.266621Z","shell.execute_reply.started":"2022-04-07T14:05:43.310656Z","shell.execute_reply":"2022-04-07T14:05:44.265777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T14:12:37.020928Z","iopub.execute_input":"2022-04-07T14:12:37.0215Z","iopub.status.idle":"2022-04-07T14:12:37.035111Z","shell.execute_reply.started":"2022-04-07T14:12:37.021462Z","shell.execute_reply":"2022-04-07T14:12:37.034121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Date format : 2019-12-25 13:30:00**","metadata":{}},{"cell_type":"code","source":"df.select(to_date(col('Bday'), 'yyyy-MM-dd HH:mm:ss').alias('Date'), to_timestamp(col('Bday'),'yyyy-MM-dd HH:mm:ss').alias('TimeStamp')).show(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T14:18:12.903764Z","iopub.execute_input":"2022-04-07T14:18:12.904153Z","iopub.status.idle":"2022-04-07T14:18:13.106102Z","shell.execute_reply.started":"2022-04-07T14:18:12.904117Z","shell.execute_reply":"2022-04-07T14:18:13.105136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Date format : 25/Dec/2019 13:30:00**","metadata":{}},{"cell_type":"code","source":"# we can't change format from the given string ..\n# the string should be of the same form to change from string to data/time type\n# df.select(to_date(col('Bday'), 'dd/MM/yyyy HH:mm:ss').alias('Date'), to_timestamp(col('Bday'),'dd/MM/yyyy HH:mm:ss').alias('TimeStamp')).show(1)\n\ndf = spark.createDataFrame([('25/Dec/2019 13:30:00',)], ['Bday'])\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T14:32:26.055158Z","iopub.execute_input":"2022-04-07T14:32:26.055462Z","iopub.status.idle":"2022-04-07T14:32:26.176284Z","shell.execute_reply.started":"2022-04-07T14:32:26.055433Z","shell.execute_reply":"2022-04-07T14:32:26.175184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.select(to_date(col('Bday'), 'dd/MMM/yyyy HH:mm:ss').alias('Date'), to_timestamp(col('Bday'),'dd/MMM/yyyy HH:mm:ss').alias('TimeStamp')).show(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T14:33:34.191201Z","iopub.execute_input":"2022-04-07T14:33:34.191537Z","iopub.status.idle":"2022-04-07T14:33:34.336127Z","shell.execute_reply.started":"2022-04-07T14:33:34.191505Z","shell.execute_reply":"2022-04-07T14:33:34.335139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Date format : 12/25/2019 01:30:00 PM**","metadata":{}},{"cell_type":"code","source":"df = spark.createDataFrame([('12/25/2019 01:30:00 PM',)], ['Bday'])\ndf.show(truncate = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T14:39:20.549966Z","iopub.execute_input":"2022-04-07T14:39:20.550272Z","iopub.status.idle":"2022-04-07T14:39:20.687969Z","shell.execute_reply.started":"2022-04-07T14:39:20.550243Z","shell.execute_reply":"2022-04-07T14:39:20.687283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if AM/PM is mentioned the timestamp hour should be in smaller case (hh)\ndf.select(to_date(col('Bday'), 'MM/dd/yyyy hh:mm:ss a').alias('Date'), to_timestamp(col('Bday'),'MM/dd/yyyy hh:mm:ss a').alias('TimeStamp')).show(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T14:42:56.122801Z","iopub.execute_input":"2022-04-07T14:42:56.12377Z","iopub.status.idle":"2022-04-07T14:42:56.302368Z","shell.execute_reply.started":"2022-04-07T14:42:56.123717Z","shell.execute_reply":"2022-04-07T14:42:56.301319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc = spark.read.csv('../input/reported-crime/Crimes_-_2001_to_Present.csv', header = True)\nnrc.show(1, truncate = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:41:59.005927Z","iopub.execute_input":"2022-04-07T17:41:59.00627Z","iopub.status.idle":"2022-04-07T17:42:00.224272Z","shell.execute_reply.started":"2022-04-07T17:41:59.00624Z","shell.execute_reply":"2022-04-07T17:42:00.223433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here original date format dd/mm/yyyy with am/pm","metadata":{"execution":{"iopub.status.busy":"2022-04-07T15:13:24.510399Z","iopub.execute_input":"2022-04-07T15:13:24.5108Z","iopub.status.idle":"2022-04-07T15:13:24.515929Z","shell.execute_reply.started":"2022-04-07T15:13:24.510763Z","shell.execute_reply":"2022-04-07T15:13:24.515078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### User defined functions (UDF)\n- self defined functions\n- these functions has to be registered in the spark to be used in all worker nodes\n- So that spark serializes the function on the driver for distribution to all executor processers\n- There is a performance penalty, so UDF's not so effective as built-in functions\n- In case of python :: spark will start a process in worker node, so it will serialize \n- It applies the function rows by row to the dataset and returns it \n- The real problem is serializing the function, that has higher computation\n- So in the backend : JVM and py4j competes for memory and can fall into \"out of memory issues\"\n- To overcome this , its better to not use UDF in python , \"but it will be fast if we write the function in JAVA or SCALA\" still we be able to use that and will be fast\n- Apache Arrow stores data in columns and can avoid serialization and de-serialization process in UDF's\n- Best to seek for built-in functions ","metadata":{}},{"cell_type":"markdown","source":"#### Working with Joins\n- \"Join expression\" decides where the two rows should join\n- \"Join type\" decides what should be the result\n- Types of Join\n    - Inner join : Only keeps key present in both\n    - outer join : keeps all the keys present in both df's\n    - left outer join : keeps all rows in left\n    - right outer join : keeps all rows in right\n- Syntax ::  **df1.join(df2, df1.column_name == df2.column_name, how = {'inner'})**","metadata":{}},{"cell_type":"code","source":"!wget -O police-station.csv https://data.cityofchicago.org/api/views/z8bn-74gv/rows.csv?accessType=DOWNLOAD\n!ls -l  ","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:41:46.429624Z","iopub.execute_input":"2022-04-12T16:41:46.430343Z","iopub.status.idle":"2022-04-12T16:41:48.784864Z","shell.execute_reply.started":"2022-04-12T16:41:46.430263Z","shell.execute_reply":"2022-04-12T16:41:48.783927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:41:48.786726Z","iopub.execute_input":"2022-04-12T16:41:48.787445Z","iopub.status.idle":"2022-04-12T16:41:49.584755Z","shell.execute_reply.started":"2022-04-12T16:41:48.787393Z","shell.execute_reply":"2022-04-12T16:41:49.583766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:41:49.587085Z","iopub.execute_input":"2022-04-12T16:41:49.587425Z","iopub.status.idle":"2022-04-12T16:41:50.347593Z","shell.execute_reply.started":"2022-04-12T16:41:49.587387Z","shell.execute_reply":"2022-04-12T16:41:50.346281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"policeStation = spark.read.csv('/kaggle/working/police-station.csv', header= True)\npoliceStation.show(1, truncate = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:41:50.349744Z","iopub.execute_input":"2022-04-12T16:41:50.350044Z","iopub.status.idle":"2022-04-12T16:41:55.800299Z","shell.execute_reply.started":"2022-04-12T16:41:50.350006Z","shell.execute_reply":"2022-04-12T16:41:55.799372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The reported crimes dataset has only the district number. Add the district name by joining with the police station dataset**","metadata":{}},{"cell_type":"code","source":"nrc.cache()\nnrc.count()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T16:29:27.126261Z","iopub.execute_input":"2022-04-07T16:29:27.126561Z","iopub.status.idle":"2022-04-07T16:31:33.388981Z","shell.execute_reply.started":"2022-04-07T16:29:27.126526Z","shell.execute_reply":"2022-04-07T16:31:33.3884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc.columns, policeStation.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-07T16:47:02.505489Z","iopub.execute_input":"2022-04-07T16:47:02.507429Z","iopub.status.idle":"2022-04-07T16:47:02.516603Z","shell.execute_reply.started":"2022-04-07T16:47:02.507391Z","shell.execute_reply":"2022-04-07T16:47:02.515688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"policeStation.select('DISTRICT').distinct().show(30, truncate = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T16:48:47.527381Z","iopub.execute_input":"2022-04-07T16:48:47.527681Z","iopub.status.idle":"2022-04-07T16:48:48.097636Z","shell.execute_reply.started":"2022-04-07T16:48:47.52765Z","shell.execute_reply":"2022-04-07T16:48:48.096757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc.select('District').distinct().show(30, truncate = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T16:49:23.905099Z","iopub.execute_input":"2022-04-07T16:49:23.905392Z","iopub.status.idle":"2022-04-07T16:49:26.582393Z","shell.execute_reply.started":"2022-04-07T16:49:23.905365Z","shell.execute_reply":"2022-04-07T16:49:26.581585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# since we need to remove the leading zeros\nfrom pyspark.sql.functions import lpad\nhelp(lpad)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T16:53:04.455866Z","iopub.execute_input":"2022-04-07T16:53:04.456267Z","iopub.status.idle":"2022-04-07T16:53:04.464671Z","shell.execute_reply.started":"2022-04-07T16:53:04.456226Z","shell.execute_reply":"2022-04-07T16:53:04.462306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc = nrc.withColumn('paddedDistrict', lpad(col('District'),3,'0'))\npoliceStation = policeStation.withColumn('paddedDistrict', lpad(col('DISTRICT'), 3, '0'))","metadata":{"execution":{"iopub.status.busy":"2022-04-07T16:59:34.686019Z","iopub.execute_input":"2022-04-07T16:59:34.686393Z","iopub.status.idle":"2022-04-07T16:59:34.726321Z","shell.execute_reply.started":"2022-04-07T16:59:34.686357Z","shell.execute_reply":"2022-04-07T16:59:34.725516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc.show(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T16:59:51.107964Z","iopub.execute_input":"2022-04-07T16:59:51.108394Z","iopub.status.idle":"2022-04-07T16:59:51.294518Z","shell.execute_reply.started":"2022-04-07T16:59:51.108355Z","shell.execute_reply":"2022-04-07T16:59:51.293642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc.select('paddedDistrict').distinct().show(30, truncate = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:00:47.62481Z","iopub.execute_input":"2022-04-07T17:00:47.625131Z","iopub.status.idle":"2022-04-07T17:00:50.937182Z","shell.execute_reply.started":"2022-04-07T17:00:47.625096Z","shell.execute_reply":"2022-04-07T17:00:50.936141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"policeStation.show(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:00:07.907492Z","iopub.execute_input":"2022-04-07T17:00:07.907829Z","iopub.status.idle":"2022-04-07T17:00:08.023414Z","shell.execute_reply.started":"2022-04-07T17:00:07.90779Z","shell.execute_reply":"2022-04-07T17:00:08.022579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"joined = nrc.join(policeStation, nrc.paddedDistrict == policeStation.paddedDistrict,'left_outer').drop('DISTRICT',\n 'ADDRESS',\n 'CITY',\n 'STATE',\n 'ZIP',\n 'WEBSITE',\n 'PHONE',\n 'FAX',\n 'TTY',\n 'X COORDINATE',\n 'Y COORDINATE',\n 'LATITUDE',\n 'LONGITUDE',\n 'LOCATION',\n 'paddedDistrict')\njoined.show(2, truncate = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:26:24.519135Z","iopub.execute_input":"2022-04-07T17:26:24.5194Z","iopub.status.idle":"2022-04-07T17:26:24.706349Z","shell.execute_reply.started":"2022-04-07T17:26:24.519366Z","shell.execute_reply":"2022-04-07T17:26:24.705436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**What is the most frequently reported non-criminal activity?**","metadata":{}},{"cell_type":"code","source":"# domestic == true\nnrc.select(col('Domestic')).distinct().show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:32:18.714738Z","iopub.execute_input":"2022-04-07T17:32:18.71538Z","iopub.status.idle":"2022-04-07T17:32:21.226684Z","shell.execute_reply.started":"2022-04-07T17:32:18.715348Z","shell.execute_reply":"2022-04-07T17:32:21.225759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc.filter(col('Domestic') == 'true').groupBy('Primary Type').count().orderBy('count', ascending = False).show(10, truncate = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:35:39.129606Z","iopub.execute_input":"2022-04-07T17:35:39.129914Z","iopub.status.idle":"2022-04-07T17:35:41.785964Z","shell.execute_reply.started":"2022-04-07T17:35:39.129881Z","shell.execute_reply":"2022-04-07T17:35:41.785131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Find Day of the week has the most number of reported crime.**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import dayofweek\nnrc1 = nrc.withColumn('Date', to_date(col('Date'), 'dd/MM/yyyy hh:mm:ss a'))\nnrc1.show(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:45:02.388181Z","iopub.execute_input":"2022-04-07T17:45:02.388481Z","iopub.status.idle":"2022-04-07T17:45:02.471391Z","shell.execute_reply.started":"2022-04-07T17:45:02.388453Z","shell.execute_reply":"2022-04-07T17:45:02.470515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"help(dayofweek)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T17:40:30.564359Z","iopub.execute_input":"2022-04-07T17:40:30.564625Z","iopub.status.idle":"2022-04-07T17:40:30.569514Z","shell.execute_reply.started":"2022-04-07T17:40:30.564597Z","shell.execute_reply":"2022-04-07T17:40:30.568829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import when, lit\nnrc1 = nrc1.withColumn('dayOfCrime', dayofweek(col('Date'))) #, when(nrc1.dayOfCrime.isNull(),lit('0')).otherwise(nrc1.dayOfCrime))\nnrc1.show(2)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T18:16:54.245917Z","iopub.execute_input":"2022-04-07T18:16:54.246942Z","iopub.status.idle":"2022-04-07T18:16:54.405849Z","shell.execute_reply.started":"2022-04-07T18:16:54.2469Z","shell.execute_reply":"2022-04-07T18:16:54.404814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc1.fillna('0', subset = ['dayOfCrime'])","metadata":{"execution":{"iopub.status.busy":"2022-04-07T18:34:36.536427Z","iopub.execute_input":"2022-04-07T18:34:36.536794Z","iopub.status.idle":"2022-04-07T18:34:36.575025Z","shell.execute_reply.started":"2022-04-07T18:34:36.536755Z","shell.execute_reply":"2022-04-07T18:34:36.574144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### NOTE: IN PYSPARK ONLY SAME DTYPE COLUMNS CAN BE REPLACED\n- string column can't be replace with int","metadata":{}},{"cell_type":"code","source":"# replacing numbers of day with string\nfrom pyspark.sql.functions import StringType\nnrc1 = nrc1.withColumn('dayOfCrime', nrc1['dayOfCrime'].cast(StringType()))\ndays = {'1':'Sunday', '2':'Monday', '3':'Tuesday', '4':'Wednesday','5':'Thursday', '6':'Friday', '7':'Saturday'}\nnrc2 = nrc1.replace(to_replace = days, subset = ['dayOfCrime'])\nnrc2.show(1, truncate  = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-07T18:38:37.252392Z","iopub.execute_input":"2022-04-07T18:38:37.252721Z","iopub.status.idle":"2022-04-07T18:38:37.562742Z","shell.execute_reply.started":"2022-04-07T18:38:37.252689Z","shell.execute_reply":"2022-04-07T18:38:37.562084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc2.cache()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T18:41:09.610391Z","iopub.execute_input":"2022-04-07T18:41:09.610957Z","iopub.status.idle":"2022-04-07T18:41:09.803786Z","shell.execute_reply.started":"2022-04-07T18:41:09.610903Z","shell.execute_reply":"2022-04-07T18:41:09.802636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nrc2.groupBy('dayOfCrime').count().orderBy('count', ascending = False).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-07T18:41:15.030673Z","iopub.execute_input":"2022-04-07T18:41:15.031238Z","iopub.status.idle":"2022-04-07T18:46:36.060143Z","shell.execute_reply.started":"2022-04-07T18:41:15.031183Z","shell.execute_reply":"2022-04-07T18:46:36.057209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### RDD (Resilient Distributed Dataset)\n- RDD : a immutable partioned collection of records that can be worked on in parallel\n- In DF each data is a structured row, but in case of RDD records are scala/Java/Python objects  \n- A low level API\n- All the DataFrame API code complies down to a RDD\n- Spark doesn't understand the inner data as it does with DataFrame\n- RDD's in scala & java will be fast \n- But running a RDD in python is similar to running a UDF function [so in this case , it has to serialized for faster performance ] \n- RDD's lack optimization seen in DF\n- Can't access built in functions with RDD's","metadata":{}},{"cell_type":"code","source":"psrdd = sc.textFile('/kaggle/working/police-station.csv')\npsrdd.first()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:03:43.747494Z","iopub.execute_input":"2022-04-12T17:03:43.747807Z","iopub.status.idle":"2022-04-12T17:03:44.532243Z","shell.execute_reply.started":"2022-04-12T17:03:43.747776Z","shell.execute_reply":"2022-04-12T17:03:44.531209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"psrdd_header  = psrdd.first()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:04:19.427784Z","iopub.execute_input":"2022-04-12T17:04:19.428075Z","iopub.status.idle":"2022-04-12T17:04:19.490826Z","shell.execute_reply.started":"2022-04-12T17:04:19.428038Z","shell.execute_reply":"2022-04-12T17:04:19.489636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps_rest = psrdd.filter(lambda line : line!= psrdd_header)\nps_rest.first()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:05:46.778823Z","iopub.execute_input":"2022-04-12T17:05:46.779157Z","iopub.status.idle":"2022-04-12T17:05:46.838203Z","shell.execute_reply.started":"2022-04-12T17:05:46.779125Z","shell.execute_reply":"2022-04-12T17:05:46.83729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**How many police stations are there ?**","metadata":{}},{"cell_type":"code","source":"ps_rest.map(lambda line: line.split(',')).collect() # this gives all the rows in the file","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:07:07.238478Z","iopub.execute_input":"2022-04-12T17:07:07.238768Z","iopub.status.idle":"2022-04-12T17:07:07.344467Z","shell.execute_reply.started":"2022-04-12T17:07:07.238739Z","shell.execute_reply":"2022-04-12T17:07:07.343459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ps_rest.map(lambda line: line.split(',')).count()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:07:41.097101Z","iopub.execute_input":"2022-04-12T17:07:41.097383Z","iopub.status.idle":"2022-04-12T17:07:41.173202Z","shell.execute_reply.started":"2022-04-12T17:07:41.097355Z","shell.execute_reply":"2022-04-12T17:07:41.172569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Display the District ID, District name, Address and Zip for the police station with District ID 7**","metadata":{}},{"cell_type":"code","source":"(ps_rest.filter(lambda line: line.split(',')[0] == '7').\n map(lambda line: (line.split(',')[0],\n                   line.split(',')[1],\n                   line.split(',')[2],\n                   line.split(',')[5],\n                  )).collect())","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:15:47.014027Z","iopub.execute_input":"2022-04-12T17:15:47.01435Z","iopub.status.idle":"2022-04-12T17:15:47.096632Z","shell.execute_reply.started":"2022-04-12T17:15:47.014296Z","shell.execute_reply":"2022-04-12T17:15:47.095709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Police stations 10 and 11 are geographically close to each other. Display the District ID, District name, address and zip code**","metadata":{}},{"cell_type":"code","source":"(ps_rest.filter(lambda line:line.split(',')[0] in ['10', '11']).\nmap(lambda line:(line.split(',')[0], \n                line.split(',')[1],\n                   line.split(',')[2],\n                   line.split(',')[5]\n                )).collect())","metadata":{"execution":{"iopub.status.busy":"2022-04-12T17:21:19.540813Z","iopub.execute_input":"2022-04-12T17:21:19.541513Z","iopub.status.idle":"2022-04-12T17:21:19.600223Z","shell.execute_reply.started":"2022-04-12T17:21:19.541476Z","shell.execute_reply":"2022-04-12T17:21:19.599394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}